from sentence_transformers import SentenceTransformer
import numpy as np
import json
import os
from datetime import datetime, timezone, timedelta
from sklearn.metrics.pairwise import cosine_similarity
from dateutil import parser as date_parser
from key_bert import extract_keywords_keybert

def load_bert_model(model_path):
    """
    BERT Sentence Embeddings ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        model_path (str): ëª¨ë¸ ê²½ë¡œ
    
    Returns:
        SentenceTransformer: ë¡œë“œëœ BERT ëª¨ë¸
    """
    return SentenceTransformer(model_path)

def load_news_data(json_path):
    """
    JSON íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        json_path (str): ë‰´ìŠ¤ ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ
    
    Returns:
        tuple: (news_titles, published_list, continent_map) ë˜ëŠ” None(ì—ëŸ¬ ë°œìƒì‹œ)
    """
    news_titles = []        # ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
    published_list = []     # ë°œí–‰ì¼ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´)
    continent_map = {}      # {ë‰´ìŠ¤ ì œëª©: ëŒ€ë¥™ëª…} ë§¤í•‘
    
    if not os.path.exists(json_path):
        print(f"âŒ {json_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
    
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # ê° ëŒ€ë¥™ì˜ ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ ì œëª©ê³¼ ë°œí–‰ì¼, ëŒ€ë¥™ëª…ì„ ì¶”ì¶œ
    for continent, cont_data in data.get("continents", {}).items():
        for source in cont_data.get("sources", []):
            for article in source.get("articles", []):
                title = article.get("title", "No Title")
                published = article.get("published", "")
                news_titles.append(title)
                published_list.append(published)
                continent_map[title] = continent
    
    if not news_titles:
        print("âŒ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    return news_titles, published_list, continent_map

def calculate_similarity_matrix(model, news_titles):
    """
    BERTë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ì œëª© ê°„ì˜ ìœ ì‚¬ë„ í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        model: BERT ëª¨ë¸
        news_titles (list): ë‰´ìŠ¤ ì œëª© ëª©ë¡
    
    Returns:
        numpy.ndarray: ìœ ì‚¬ë„ í–‰ë ¬
    """
    embeddings = model.encode(news_titles)
    return cosine_similarity(embeddings)

def perform_clustering(similarity_matrix, threshold=0.5):
    """
    ìœ ì‚¬ë„ í–‰ë ¬ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        similarity_matrix (numpy.ndarray): ìœ ì‚¬ë„ í–‰ë ¬
        threshold (float): í´ëŸ¬ìŠ¤í„°ë§ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.5)
    
    Returns:
        tuple: (clusters, processed_indices)
            - clusters: í´ëŸ¬ìŠ¤í„° ëª©ë¡
            - processed_indices: ì²˜ë¦¬ëœ ë‰´ìŠ¤ ì¸ë±ìŠ¤ ì§‘í•©
    """
    clusters = []
    processed_indices = set()
    
    for i, row in enumerate(similarity_matrix):
        if i in processed_indices:
            continue
        cluster = {i}
        for j, sim in enumerate(row):
            if i != j and sim > threshold and j not in processed_indices:
                cluster.add(j)
        if len(cluster) > 1:
            clusters.append(cluster)
            processed_indices.update(cluster)
            
    return clusters, processed_indices

def categorize_clusters(clusters, news_titles, continent_map):
    """
    í´ëŸ¬ìŠ¤í„°ë¥¼ ëŒ€ë¥™ ê¸°ì¤€ìœ¼ë¡œ ê¸€ë¡œë²Œ/ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    
    Args:
        clusters (list): í´ëŸ¬ìŠ¤í„° ëª©ë¡
        news_titles (list): ë‰´ìŠ¤ ì œëª© ëª©ë¡
        continent_map (dict): ë‰´ìŠ¤ ì œëª©ë³„ ëŒ€ë¥™ ì •ë³´
    
    Returns:
        tuple: (global_clusters, regional_clusters)
            - global_clusters: ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„° ëª©ë¡ (2ê°œ ì´ìƒ ëŒ€ë¥™ í¬í•¨)
            - regional_clusters: ì§€ì—­ í´ëŸ¬ìŠ¤í„° ëª©ë¡ (ë‹¨ì¼ ëŒ€ë¥™)
    """
    global_clusters = []  # 2ê°œ ì´ìƒì˜ ëŒ€ë¥™ì´ í¬í•¨ëœ í´ëŸ¬ìŠ¤í„° (ê¸€ë¡œë²Œ íŠ¸ë Œë“œ)
    regional_clusters = []  # í•œ ëŒ€ë¥™ë§Œ í¬í•¨ëœ í´ëŸ¬ìŠ¤í„° (ì§€ì—­ íŠ¸ë Œë“œ)
    
    for cluster in clusters:
        continents_in_cluster = set()
        for idx in cluster:
            title = news_titles[idx]
            continents_in_cluster.add(continent_map.get(title, "Unknown"))
        if len(continents_in_cluster) >= 2:
            global_clusters.append((cluster, continents_in_cluster))
        else:
            regional_clusters.append((cluster, continents_in_cluster))
            
    return global_clusters, regional_clusters

def extract_leftover_news(news_count, processed_indices, continent_map, news_titles, published_list):
    """
    í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ë˜ì§€ ì•Šì€ ì”ì—¬ ë‰´ìŠ¤ë¥¼ ì¶”ì¶œí•˜ê³  ëŒ€ë¥™ë³„ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        news_count (int): ì´ ë‰´ìŠ¤ ê°œìˆ˜
        processed_indices (set): í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ëœ ë‰´ìŠ¤ ì¸ë±ìŠ¤
        continent_map (dict): ë‰´ìŠ¤ ì œëª©ë³„ ëŒ€ë¥™ ì •ë³´
        news_titles (list): ë‰´ìŠ¤ ì œëª© ëª©ë¡
        published_list (list): ë°œí–‰ì¼ ëª©ë¡
    
    Returns:
        dict: ëŒ€ë¥™ë³„ ì£¼ìš” ë‰´ìŠ¤ ì¸ë±ìŠ¤ (ìµœì‹ ìˆœ ìµœëŒ€ 3ê°œ)
    """
    all_indices = set(range(news_count))
    leftover_indices = all_indices - processed_indices
    
    # ê° ëŒ€ë¥™ë³„ë¡œ ì”ì—¬ ë‰´ìŠ¤ ì¸ë±ìŠ¤ ê·¸ë£¹í™”
    leftover_by_continent = {}
    for idx in leftover_indices:
        cont = continent_map.get(news_titles[idx], "Unknown")
        leftover_by_continent.setdefault(cont, []).append(idx)
    
    # ê° ëŒ€ë¥™ë³„ë¡œ ìµœì‹  ë‰´ìŠ¤ 3ê°œ ì„ íƒ (ë°œí–‰ì¼ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
    highlight_by_continent = {}
    for cont, indices in leftover_by_continent.items():
        def parse_date(idx):
            try:
                return date_parser.parse(published_list[idx])
            except Exception:
                return datetime.min
        sorted_indices = sorted(indices, key=parse_date, reverse=True)
        highlight_by_continent[cont] = sorted_indices[:3]
        
    return highlight_by_continent

def select_most_important_news(cluster_news, news_titles, model):
    """
    í´ëŸ¬ìŠ¤í„°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ 1ê°œë¥¼ ì„ ì •í•©ë‹ˆë‹¤.
    BERT ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬(centroid)ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë‰´ìŠ¤ ì„ íƒí•©ë‹ˆë‹¤.
    
    Args:
        cluster_news (set): í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ëœ ë‰´ìŠ¤ ì¸ë±ìŠ¤ ì§‘í•©
        news_titles (list): ë‰´ìŠ¤ ì œëª© ëª©ë¡
        model: BERT ëª¨ë¸
    
    Returns:
        int: ì„ ì •ëœ ì¤‘ìš” ë‰´ìŠ¤ì˜ ì¸ë±ìŠ¤
    """
    cluster_news = list(cluster_news)  # setì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    if len(cluster_news) == 1:
        return cluster_news[0]  # ë‰´ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    embeddings = model.encode([news_titles[idx] for idx in cluster_news])
    centroid = np.mean(embeddings, axis=0)
    similarities = np.dot(embeddings, centroid) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(centroid))
    best_idx = np.argmax(similarities)
    
    return cluster_news[best_idx]  # ê°€ì¥ ì¤‘ì‹¬ì ì¸ ë‰´ìŠ¤ ë°˜í™˜

def print_global_trends(global_clusters, news_titles, continent_map, model):
    """
    ê¸€ë¡œë²Œ íŠ¸ë Œë“œ í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        global_clusters (list): ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„° ëª©ë¡
        news_titles (list): ë‰´ìŠ¤ ì œëª© ëª©ë¡
        continent_map (dict): ë‰´ìŠ¤ ì œëª©ë³„ ëŒ€ë¥™ ì •ë³´
        model: BERT ëª¨ë¸
    """
    print("\nGlobal economic trends (multi-continent clusters)")
    if global_clusters:
        for i, (cluster, conts) in enumerate(global_clusters, start=1):
            print(f"\nGlobal Cluster {i} [Continent: {', '.join(conts)}]:")
            continent_news = {}
            for idx in cluster:
                cont = continent_map.get(news_titles[idx], "Unknown")
                continent_news.setdefault(cont, []).append(idx)

            for cont, indices in continent_news.items():
                important_idx = select_most_important_news(indices, news_titles, model)
                print(f"- {news_titles[important_idx]} [{cont}]")
    else:
        print("ê¸€ë¡œë²Œ íŠ¸ë Œë“œ í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def print_regional_trends(regional_clusters, news_titles, model):
    """
    ì§€ì—­ íŠ¸ë Œë“œ í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        regional_clusters (list): ì§€ì—­ í´ëŸ¬ìŠ¤í„° ëª©ë¡
        news_titles (list): ë‰´ìŠ¤ ì œëª© ëª©ë¡
        model: BERT ëª¨ë¸
    """
    print("\nRegionally significant trends (single continent clusters)")
    if regional_clusters:
        for i, (cluster, conts) in enumerate(regional_clusters, start=1):
            cont_label = list(conts)[0] if conts else "Unknown"
            print(f"\nRegional clusters {i} [{cont_label}]:")
            
            # ì¤‘ìš” ë‰´ìŠ¤ 1ê°œ ì„ íƒ
            important_idx = select_most_important_news(cluster, news_titles, model)  
            print(f"- {news_titles[important_idx]} [{cont_label}]")  

            # ì¤‘ìš” ë‰´ìŠ¤ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë‰´ìŠ¤ ì œëª© ëª¨ìŒ
            remaining_news = [news_titles[idx] for idx in cluster if idx != important_idx]

            if remaining_news:
                # KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œ ê¸€ì í‚¤ì›Œë“œ ì œê±°ë¨)
                keywords = extract_keywords_keybert(" ".join(remaining_news), top_n=5)  
                
                # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‰¼í‘œë¡œ ì—°ê²°
                keywords_str = ", ".join(keywords) if keywords else "í‚¤ì›Œë“œ ì—†ìŒ"
                print(f"Other news keywords: {keywords_str}")
            else:
                print("  ğŸ” ì£¼ìš” í‚¤ì›Œë“œ: ì—†ìŒ")

def print_regional_highlights(highlight_by_continent, news_titles, published_list):
    """
    ëŒ€ë¥™ë³„ ì£¼ìš” ë‰´ìŠ¤ í•˜ì´ë¼ì´íŠ¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        highlight_by_continent (dict): ëŒ€ë¥™ë³„ ì£¼ìš” ë‰´ìŠ¤ ì¸ë±ìŠ¤
        news_titles (list): ë‰´ìŠ¤ ì œëª© ëª©ë¡
        published_list (list): ë°œí–‰ì¼ ëª©ë¡
    """
    print("\nHighlights by region (unclustered residual news)")
    for cont, indices in highlight_by_continent.items():
        print(f"\n{cont} Highlight:")
        for idx in indices:
            print(f"- {news_titles[idx]} (Publication Date: {published_list[idx]})")

def print_timestamp():
    """
    í˜„ì¬ í•œêµ­ ì‹œê°„ì„ í¬ë§·ì— ë§ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    kst = timezone(timedelta(hours=9))
    kst_time = datetime.now()
    formatted_time = kst_time.strftime('%Y-%m-%d %H:%M UTC')
    print(f"ğŸŒ ê¸€ë¡œë²Œ ê²½ì œ íŠ¸ë Œë“œ ë¶„ì„\n{formatted_time}\n#ê¸€ë¡œë²Œê²½ì œ\n\n")

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜: ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œë¶€í„° ë¶„ì„ ê²°ê³¼ ì¶œë ¥ê¹Œì§€ì˜ ì „ì²´ ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    model_path = "/Users/rokkie/Python_Project/Telegram_API/binance/future/News/similarity-search/Model/all-MiniLM-L6-v2"
    json_path = "/Users/rokkie/Python_Project/Telegram_API/binance/future/News/RSS/result.json"
    
    model = load_bert_model(model_path)
    data_result = load_news_data(json_path)
    
    if data_result is None:
        return
    
    news_titles, published_list, continent_map = data_result
    
    # 2. BERT ì„ë² ë”© ë° ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    similarity_matrix = calculate_similarity_matrix(model, news_titles)
    print("ğŸ” BERT ê¸°ë°˜ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° ì™„ë£Œ")
    
    # 3. ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
    clusters, processed_indices = perform_clustering(similarity_matrix, threshold=0.5)
    
    # 4. í´ëŸ¬ìŠ¤í„°ë³„ ë¶„ë¥˜
    global_clusters, regional_clusters = categorize_clusters(clusters, news_titles, continent_map)
    
    # 5. ì”ì—¬ ë‰´ìŠ¤ ì¶”ì¶œ ë° ëŒ€ë¥™ë³„ ì •ë¦¬
    highlight_by_continent = extract_leftover_news(
        len(news_titles), processed_indices, continent_map, news_titles, published_list
    )
    
    # 6. ê²°ê³¼ ì¶œë ¥
    print_global_trends(global_clusters, news_titles, continent_map, model)
    print_regional_trends(regional_clusters, news_titles, model)
    print_regional_highlights(highlight_by_continent, news_titles, published_list)
    print_timestamp()

if __name__ == "__main__":
    main()