from sentence_transformers import SentenceTransformer
import numpy as np
import json
import os
import pandas as pd
from datetime import datetime, timezone, timedelta
from sklearn.metrics.pairwise import cosine_similarity
from dateutil import parser as date_parser  # ë‚ ì§œ íŒŒì‹±ìš©
from key_bert import extract_keywords_keybert  # KeyBERT í•¨ìˆ˜ import

# ğŸ”¹ BERT Sentence Embeddings ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

news_data_path = "./data/political_rss.csv"
news_data = pd.read_csv(news_data_path, encoding='utf-8')

# ê²°ê³¼ ë°ì´í„° ë° ë‰´ìŠ¤ ëª©ë¡ ì´ˆê¸°í™”
news_titles = []         # ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
published_list = []      # ë°œí–‰ì¼ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´)
continent_map = {}       # {ë‰´ìŠ¤ ì œëª©: ëŒ€ë¥™ëª…} ë§¤í•‘


# 2. BERT ì„ë² ë”© ë° ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
embeddings = model.encode(news_titles)
similarity_matrix = cosine_similarity(embeddings)
print("ğŸ” BERT ê¸°ë°˜ ìœ ì‚¬ë„ í–‰ë ¬:")

# 3. ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ (ì„ê³„ê°’: 0.5)
threshold = 0.5
clusters = []
processed_indices = set()

for i, row in enumerate(similarity_matrix):
    if i in processed_indices:
        continue
    cluster = {i}
    for j, sim in enumerate(row):
        if i != j and sim > threshold and j not in processed_indices:
            cluster.add(j)
    if len(cluster) > 1:
        clusters.append(cluster)
        processed_indices.update(cluster)

# 4. í´ëŸ¬ìŠ¤í„°ë³„ ë¶„ë¥˜ (ë‹¤ì¤‘ ëŒ€ë¥™ í´ëŸ¬ìŠ¤í„° vs ë‹¨ì¼ ëŒ€ë¥™ í´ëŸ¬ìŠ¤í„°)
global_clusters = []  # 2ê°œ ì´ìƒì˜ ëŒ€ë¥™ì´ í¬í•¨ëœ í´ëŸ¬ìŠ¤í„° (ê¸€ë¡œë²Œ íŠ¸ë Œë“œ)
regional_clusters = []  # í•œ ëŒ€ë¥™ë§Œ í¬í•¨ëœ í´ëŸ¬ìŠ¤í„° (ì§€ì—­ íŠ¸ë Œë“œ)

for cluster in clusters:
    continents_in_cluster = set()
    for idx in cluster:
        title = news_titles[idx]
        continents_in_cluster.add(continent_map.get(title, "Unknown"))
    if len(continents_in_cluster) >= 2:
        global_clusters.append((cluster, continents_in_cluster))
    else:
        regional_clusters.append((cluster, continents_in_cluster))

# 5. ì”ì—¬(í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ì§€ ì•Šì€) ë‰´ìŠ¤ ì¶”ì¶œ ë° ëŒ€ë¥™ë³„ ì •ë¦¬
all_indices = set(range(len(news_titles)))
leftover_indices = all_indices - processed_indices
# ê° ëŒ€ë¥™ë³„ë¡œ ì”ì—¬ ë‰´ìŠ¤ ì¸ë±ìŠ¤ ê·¸ë£¹í™”
leftover_by_continent = {}
for idx in leftover_indices:
    cont = continent_map.get(news_titles[idx], "Unknown")
    leftover_by_continent.setdefault(cont, []).append(idx)

# ê° ëŒ€ë¥™ë³„ë¡œ ìµœì‹  ë‰´ìŠ¤ 3ê°œ ì„ íƒ (ë°œí–‰ì¼ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
highlight_by_continent = {}
for cont, indices in leftover_by_continent.items():
    def parse_date(idx):
        try:
            return date_parser.parse(published_list[idx])
        except Exception:
            return datetime.min
    sorted_indices = sorted(indices, key=parse_date, reverse=True)
    highlight_by_continent[cont] = sorted_indices[:3]

# 6. ì¤‘ìš” ë‰´ìŠ¤ ì„ ì • í•¨ìˆ˜ (BERT ê¸°ë°˜)
def select_most_important_news(cluster_news):
    """
    í´ëŸ¬ìŠ¤í„°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ 1ê°œë¥¼ ì„ ì •í•˜ëŠ” í•¨ìˆ˜
    - ë°©ë²•: BERT ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬(centroid)ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë‰´ìŠ¤ ì„ íƒ
    """
    cluster_news = list(cluster_news)  # ğŸ”¹ setì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¤‘ìš”)
    
    if len(cluster_news) == 1:
        return cluster_news[0]  # ë‰´ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    embeddings = model.encode([news_titles[idx] for idx in cluster_news])
    centroid = np.mean(embeddings, axis=0)
    similarities = np.dot(embeddings, centroid) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(centroid))
    best_idx = np.argmax(similarities)
    
    return cluster_news[best_idx]  # ê°€ì¥ ì¤‘ì‹¬ì ì¸ ë‰´ìŠ¤ ë°˜í™˜

# 7. ê²°ê³¼ ì¶œë ¥
print("\nGlobal economic trends (multi-continent clusters)")
if global_clusters:
    for i, (cluster, conts) in enumerate(global_clusters, start=1):
        print(f"\nGlobal Cluster {i} [Continent: {', '.join(conts)}]:")
        continent_news = {}
        for idx in cluster:
            cont = continent_map.get(news_titles[idx], "Unknown")
            continent_news.setdefault(cont, []).append(idx)

        for cont, indices in continent_news.items():
            important_idx = select_most_important_news(indices)
            print(f"- {news_titles[important_idx]} [{cont}]")
else:
    print("ê¸€ë¡œë²Œ íŠ¸ë Œë“œ í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

print("\nRegionally significant trends (single continent clusters)")
if regional_clusters:
    for i, (cluster, conts) in enumerate(regional_clusters, start=1):
        cont_label = list(conts)[0] if conts else "Unknown"
        print(f"\nRegional clusters {i} [{cont_label}]:")
        
        # ğŸ”¹ ì¤‘ìš” ë‰´ìŠ¤ 1ê°œ ì„ íƒ
        important_idx = select_most_important_news(cluster)  
        print(f"- {news_titles[important_idx]} [{cont_label}]")  

        # ğŸ”¹ ì¤‘ìš” ë‰´ìŠ¤ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë‰´ìŠ¤ ì œëª© ëª¨ìŒ
        remaining_news = [news_titles[idx] for idx in cluster if idx != important_idx]

        if remaining_news:
            # ğŸ”¹ KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œ ê¸€ì í‚¤ì›Œë“œ ì œê±°ë¨)
            keywords = extract_keywords_keybert(" ".join(remaining_news), top_n=5)  
            
            # ğŸ“Œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‰¼í‘œë¡œ ì—°ê²°
            keywords_str = ", ".join(keywords) if keywords else "í‚¤ì›Œë“œ ì—†ìŒ"
            print(f"Other news keywords: {keywords_str}")
        else:
            print("  ğŸ” ì£¼ìš” í‚¤ì›Œë“œ: ì—†ìŒ")

print("\nHighlights by region (unclustered residual news)")
for cont, indices in highlight_by_continent.items():
    print(f"\n{cont} Highlight:")
    for idx in indices:
        print(f"- {news_titles[idx]} (Publication Date: {published_list[idx]})")

# í•œêµ­ ì‹œê°„ (KST, UTC+9) ì •ì˜
kst = timezone(timedelta(hours=9))

# KST ê¸°ì¤€ ë‚ ì§œ ìƒì„±
kst_time = datetime.now()

# ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
formatted_time = kst_time.strftime('%Y-%m-%d %H:%M UTC')
print(f"ğŸŒ ê¸€ë¡œë²Œ ê²½ì œ íŠ¸ë Œë“œ ë¶„ì„\n{formatted_time}\n#ê¸€ë¡œë²Œê²½ì œ\n\n")